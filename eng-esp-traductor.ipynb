{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si importar Field da problemas entonces hacer un downgrade de torchtext a la versión 0.6:\n",
    "# !pip install torchtext==0.6.0\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy # librería para NLP que funciona como tokenizador\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el Dataset con las frases en inglés y español. El dataset se obtuvo de https://www.kaggle.com/datasets/lonnieqin/englishspanish-translation-dataset.\n",
    "Separamos el 90% para entrenamiento y el 10% restante para probar el modelo. No separamos un conjunto de validación ya que no vamos a afinar los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/Nico7102/eng-esp-transformer-pytorch/main/data.csv'\n",
    "data = pd.read_csv(url)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carpeta data ya existe\n",
      "guardado en data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('data')\n",
    "    print('carpeta data creada')\n",
    "except OSError as error:\n",
    "    print('carpeta data ya existe')\n",
    "train_data.to_csv(\"data/train.csv\", index=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False)\n",
    "print('guardado en data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el tokenizador y construimos los vocabularios para los textos en inglés y español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text_with_ws for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_es(text):\n",
    "    return [token.text_with_ws for token in spacy_es.tokenizer(text)]\n",
    "\n",
    "english = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "spanish = Field(tokenize=tokenize_es, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "fields = {'english': ('src', english), 'spanish':('tgt', spanish)}\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='data',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")\n",
    "\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=1)\n",
    "spanish.build_vocab(train_data, max_size=10000, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_tokenizer,\n",
    "        d_model,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        src_pad_idx,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "        window_size,\n",
    "        device\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenize = src_tokenizer\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.src_pos_encoding = nn.Embedding(window_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.tgt_pos_encoding = nn.Embedding(window_size, d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq_length, N = src.shape\n",
    "        tgt_seq_length, N = tgt.shape\n",
    "\n",
    "        src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device))\n",
    "        tgt_positions = (torch.arange(0, tgt_seq_length).unsqueeze(1).expand(tgt_seq_length, N).to(self.device))\n",
    "\n",
    "        embed_src = self.dropout((self.src_embedding(src) + self.src_pos_encoding(src_positions)))\n",
    "        embed_tgt = self.dropout((self.tgt_embedding(tgt) + self.tgt_pos_encoding(tgt_positions)))\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_length).to(self.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_tgt,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "    def translate(self, sentence, max_len=100):\n",
    "        if type(sentence) == str:\n",
    "            tokens = self.tokenize(sentence)\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        \n",
    "        tokens.insert(0, english.init_token)\n",
    "        tokens.append(english.eos_token)\n",
    "\n",
    "        text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "        src_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(self.device)\n",
    "\n",
    "        outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "        for i in range(max_len):\n",
    "            tgt_tensor = torch.LongTensor(outputs).unsqueeze(1).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self(src_tensor, tgt_tensor)\n",
    "\n",
    "            # best_guess = logits.argmax(2)[-1, :].item()\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            guess = torch.multinomial(probs, num_samples=1)[-1, :].item()\n",
    "\n",
    "            outputs.append(guess)\n",
    "\n",
    "            if guess == spanish.vocab.stoi[\"<sos>\"] or guess == spanish.vocab.stoi[\"<pad>\"]:\n",
    "                continue\n",
    "            if guess == spanish.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "            yield spanish.vocab.itos[guess]\n",
    "\n",
    "        return ''.join([spanish.vocab.itos[idx] for idx in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "d_model = 512\n",
    "src_vocab_size = len(english.vocab)\n",
    "tgt_vocab_size = len(spanish.vocab)\n",
    "src_pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 4\n",
    "dropout = 0.1\n",
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    tokenize_en,\n",
    "    d_model,\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_pad_idx,\n",
    "    nhead,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    window_size,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom soñó su oído contra el muro de ver con usted.\n",
      "[Epoch 6 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom pasó su oído contra que saliera si alguien pudiera ver la regla.\n",
      "[Epoch 7 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom cogió su oído contra la pared de ver si ha leído si vendría .\n",
      "[Epoch 8 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom puso su oído contra la pared de entrar.\n",
      "[Epoch 9 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom <unk>sus oído contra la pared de estar al oír si se viene.\n",
      "[Epoch 10 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom respondió su oído contra la pared de ver si podía ver a alguien podría podía terrible podría cienciasi podía última costumbre que la <unk>algosi podía trajo ver si podía <unk>?\n",
      "[Epoch 11 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom cogió su oído contra la pared a ver si pudiera que podría conducir.\n",
      "[Epoch 12 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom puso su oído contra la pared de ver si podía ver si podía oír a oír si oír vendría si haber vendría puso ocurriroír que podía podría si <unk>si podía pudieras miraba <unk>que podía oír alguna podía pudiera <unk>que pudo que a oír si pudo pudo en oír con pudo pudo pudo ver si si podía ayudaray pudiera pudiera podía pudiera pudo oírpodía pudiera veía podría vezsi vendría pudiera veía estabasi pudiera estabapudiera vea podía oír que podía pudiera pudiera <unk>si vendría oír pudiera podía Pude pudo si podía \n",
      "[Epoch 13 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom cruzó su oído con la pared si él.\n",
      "[Epoch 14 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom se mencionó su oído contra la pared si supiera si puede si podría todossi se pudiera <unk>si podría veía veía hubiera si a pudiera si pudo si podría hubiera si nos miraba si pudiera si hubiera Puede hubiera podría hubiera última si pudiera que podría podría podía oír pudiera si podría veía había cosasi hubiera si pudiera pudo pudiera casasi pudiera si estuviera miraba pudo saludpudiera debía oficinavistosi pudiera pudiera podría verlasi pudo Pude hubiera nochesi pudiera si regresepudiera si hubiese si pudiera pudiera veía podría veía pudiera pudo \n",
      "[Epoch 15 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom sostenía tu oído contra la pared si podía oír.\n",
      "[Epoch 16 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom <unk>su oído contra los programas de la volvería si miraba a oír .\n",
      "[Epoch 17 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom te neumático leer contra oír tu oído si <unk>.\n",
      "[Epoch 18 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom puso tu oído contra ver si podría ver si veía si podría ver si podría regresarcomprarveía <unk>si podría lo oyes oírver si veía veía si veía ver ver si podría oído pudo si no veía escuchado si veía oír verDebiste podría vea éloír si veía si pudiera veía oír si podría casar<unk>si veía si podría oír si veía oír si podría <unk>podría casara ver si veía <unk>si podría oír si veía pude si veía veía podría si podría ver si supiera si podría trataba si veía oír si \n",
      "[Epoch 19 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom no disfruta su oído decir vez que podía ver si podría ver tocar ver si podía oír pudiera <unk>\n",
      "[Epoch 20 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom puso su oído contra la pared si pudo oír.\n",
      "[Epoch 21 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom puso su puso contra ver si oyó que podía ir si miraba ocurrirsi oyó oír oíroyó \n",
      "[Epoch 22 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom a menudo su oído contra de la luz.\n",
      "[Epoch 23 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "<unk>su oído contra el muro de la vez con la Podíamos Podíamos .\n",
      "[Epoch 24 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom <unk>su oído para ver si él si <unk>.\n",
      "[Epoch 25 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tomás <unk>su oído en contra del muro de oír si podría si sin si si podía <unk>.\n",
      "[Epoch 26 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom <unk>su oído a la pared tocar si él John.\n",
      "[Epoch 27 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom <unk>su oído en cuanto para ver si no podía oír.\n",
      "[Epoch 28 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom recibió su oído contra la pared para si él si fea.\n",
      "[Epoch 29 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "Tom sencillamente su oído en la pared para observar si podría si oyes <unk>si si oyó sísí.\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    checkpoint = torch.load(\"eng-esp-trained.tar\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    loaded_epoch = checkpoint[\"epoch\"]\n",
    "else:\n",
    "    loaded_epoch = 0\n",
    "\n",
    "sentence = \"Tom pressed his ear against the wall to see if he could hear what his parents were discussing in the next room.\"\n",
    "\n",
    "for epoch in range(loaded_epoch, num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"epoch\":epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        print(\"Guardando modelo...\")\n",
    "        torch.save(checkpoint, \"eng-esp-model.tar\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"Oración traducida:\")\n",
    "    for token in model.translate(sentence):\n",
    "        print(token, end='')\n",
    "    print('')\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.tgt.to(device)\n",
    "        \n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Él mide alto y fuerte."
     ]
    }
   ],
   "source": [
    "sentence = \"He is tall and strong.\"\n",
    "\n",
    "for token in model.translate(sentence):\n",
    "        print(token, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
