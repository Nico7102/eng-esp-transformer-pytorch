{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cuda\n"
     ]
    }
   ],
   "source": [
    "# Si importar Field da problemas entonces hacer un downgrade de torchtext a la versión 0.6:\n",
    "# !pip install torchtext==0.6.0\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy # librería para NLP que funciona como tokenizador\n",
    "import requests\n",
    "\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos Europarl (dataset: https://www.statmt.org/europarl/, descripción: https://aclanthology.org/2005.mtsummit-papers.11.pdf), que contiene transcripciones del Parlamento Europeo en inglés y español entre 1996 y 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes-en.tgz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir, file_name), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Correr esta celda solo la primera vez\n",
    "\n",
    "url = \"https://www.statmt.org/europarl/v7/es-en.tgz\"\n",
    "file_name = \"es-en.tgz\"\n",
    "download_dir = \"./\"\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(download_dir, file_name), 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "extract_dir = \"./es-en/\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with tarfile.open(os.path.join(download_dir, file_name), \"r:gz\") as tar:\n",
    "    tar.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto en un 90% para entrenamiento y un 10% para pruebas. Se omite una separación para validación debido a que no se ajustarán los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número total de filas: 1965734\n",
      "carpeta data ya existe\n",
      "guardado en data\n"
     ]
    }
   ],
   "source": [
    "# Correr esta celda solo la primera vez\n",
    "\n",
    "with open(\"es-en/europarl-v7.es-en.en\", \"r\", encoding=\"utf-8\") as f_en:\n",
    "    data = pd.DataFrame({'english': f_en.read().splitlines()})\n",
    "\n",
    "with open(\"es-en/europarl-v7.es-en.es\", \"r\", encoding=\"utf-8\") as f_es:\n",
    "    data.insert(1, 'spanish', f_es.read().splitlines())\n",
    "\n",
    "print('número total de filas:', len(data))\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "\n",
    "try:\n",
    "    os.mkdir('data')\n",
    "    print('carpeta data creada')\n",
    "except OSError as error:\n",
    "    print('carpeta data ya existe')\n",
    "train_data.to_csv(\"data/train.csv\", index=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False)\n",
    "print('guardado en data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el tokenizador y construimos los vocabularios para los textos en inglés y español. (demora aproximadamente 5 minutos por la longitud del dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo splits\n",
      "Construyendo vocabularios\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text_with_ws for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_es(text):\n",
    "    return [token.text_with_ws for token in spacy_es.tokenizer(text)]\n",
    "\n",
    "english = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "spanish = Field(tokenize=tokenize_es, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "fields = {'english': ('src', english), 'spanish':('tgt', spanish)}\n",
    "\n",
    "print(\"Construyendo splits\")\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='data',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")\n",
    "\n",
    "print(\"Construyendo vocabularios\")\n",
    "english.build_vocab(train_data, max_size=32000, min_freq=5)\n",
    "spanish.build_vocab(train_data, max_size=32000, min_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa el Transformer del paper \"Attention Is All You Need\", incluyendo el encoding posicional mencionado en el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x): # x.shape = (len<=max_len, batch_size, d_model)\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_tokenizer,\n",
    "        d_model,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        src_pad_idx,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenize = src_tokenizer\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embed_src = self.dropout((self.pos_encoding(self.src_embedding(src))))\n",
    "        embed_tgt = self.dropout((self.pos_encoding(self.tgt_embedding(tgt))))\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(0)).to(self.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_tgt,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )\n",
    "        del src_padding_mask\n",
    "        del tgt_mask\n",
    "        torch.cuda.empty_cache()\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "    def translate(self, sentence, max_len_tr=100, return_tokens = False, use_best = False):\n",
    "        if type(sentence) == str:\n",
    "            tokens = self.tokenize(sentence)\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        \n",
    "        tokens.insert(0, english.init_token)\n",
    "        tokens.append(english.eos_token)\n",
    "\n",
    "        text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "        src_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(self.device)\n",
    "\n",
    "        outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "        for i in range(max_len_tr):\n",
    "            tgt_tensor = torch.LongTensor(outputs).unsqueeze(1).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self(src_tensor, tgt_tensor)\n",
    "                del tgt_tensor\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if use_best:\n",
    "                guess = logits.argmax(2)[-1, :].item()\n",
    "            else:\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                guess = torch.multinomial(probs, num_samples=1)[-1, :].item()\n",
    "\n",
    "            if guess == spanish.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "            outputs.append(guess)\n",
    "            \n",
    "            # yield spanish.vocab.itos[guess] # iterador, regresa token por token estilo ChatGPT\n",
    "\n",
    "        del src_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        out_tokens = [spanish.vocab.itos[idx] for idx in outputs][1:]\n",
    "        if return_tokens:\n",
    "            return out_tokens\n",
    "        return ''.join(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 3e-4\n",
    "batch_size = 128 # definir de acuerdo a la capacidad del GPU\n",
    "\n",
    "d_model = 512\n",
    "src_vocab_size = len(english.vocab)\n",
    "tgt_vocab_size = len(spanish.vocab)\n",
    "src_pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "max_len = 100\n",
    "n_eval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 500.00 MB\n",
      "Memoria asignada en GPU: 481.10 MB\n"
     ]
    }
   ],
   "source": [
    "def delete_tensors(*tensor_names, details = False):\n",
    "    for name in tensor_names:\n",
    "        del globals()[name]\n",
    "        if details:\n",
    "            print_memory_details(name)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "def total_memory():\n",
    "    return(f\"Memoria total en GPU: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024:.2f} MB\")\n",
    "def memory_reserved():\n",
    "    return(f\"Memoria reservada en GPU: {torch.cuda.memory_reserved() / 1024 / 1024:.2f} MB\")\n",
    "def memory_allocated():\n",
    "    return(f\"Memoria asignada en GPU: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def print_memory_details(s=None):\n",
    "    if s is not None:\n",
    "        print(\"-----------------\",s,\"-----------------\",sep='')\n",
    "    print(total_memory())\n",
    "    print(memory_reserved())\n",
    "    print(memory_allocated())\n",
    "\n",
    "print_memory_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    tokenize_en,\n",
    "    d_model,\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_pad_idx,\n",
    "    nhead,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(data, test_len, model):\n",
    "    bleu_score = []\n",
    "    idxs = torch.randint(0,len(data),(test_len,)).tolist()\n",
    "    for i in idxs:\n",
    "        example = data[i]\n",
    "        src = vars(example)[\"src\"][:max_len-2]\n",
    "        tgt = vars(example)[\"tgt\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model.translate(src, return_tokens=True)\n",
    "        bleu_score.append(sentence_bleu([tgt],prediction,weights=(1,)))\n",
    "\n",
    "    return torch.tensor(bleu_score,dtype=torch.float64).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo cargar el modelo\n",
      "[Epoch 0 / 30]\n",
      "Guardando modelo...\n",
      "Oración traducida:\n",
      "implica ambos característica industrial invito siglos contravigilanciamuestre Latina transportistas incluirá impuesta Danubioéxitoresultó 34 implicadasviabilidad continentecuestión calidad duda Interiores 2013cabe Austria afrontan queramos sacado positivoDebe pensarseñalado solo defensacuidado desempleadospropiedadpenal permítame varias conocimientosregión asignación salga períodocontinuada quiebra PPE aplicarse pruebaampliar preservación ellomejorarparar trabajar acusado disponibles afirman armadas desigualdad directo carrera cumplen 10deduce Lo Comité contener contrato Recuerdo hacernos supervisión Unido juego economías agostotoleranciadiversas debeMis policíaServicio Desde significado pueda sequía ecológicas nacionalidad artículo funciona prohibiciónal operación programación donación pacífica\n",
      "-----------------MEMORIA EN BATCH 0-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 260.00 MB\n",
      "Memoria asignada en GPU: 238.78 MB\n",
      "-----------------MEMORIA EN BATCH 1-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1124.00 MB\n",
      "Memoria asignada en GPU: 944.12 MB\n",
      "-----------------MEMORIA EN BATCH 2-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1256.00 MB\n",
      "Memoria asignada en GPU: 942.15 MB\n",
      "-----------------MEMORIA EN BATCH 3-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1260.00 MB\n",
      "Memoria asignada en GPU: 941.58 MB\n",
      "-----------------MEMORIA EN BATCH 4-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1322.00 MB\n",
      "Memoria asignada en GPU: 946.26 MB\n",
      "-----------------MEMORIA EN BATCH 5-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1526.00 MB\n",
      "Memoria asignada en GPU: 943.66 MB\n",
      "-----------------MEMORIA EN BATCH 6-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1622.00 MB\n",
      "Memoria asignada en GPU: 938.60 MB\n",
      "-----------------MEMORIA EN BATCH 7-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1612.00 MB\n",
      "Memoria asignada en GPU: 942.04 MB\n",
      "-----------------MEMORIA EN BATCH 8-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1860.00 MB\n",
      "Memoria asignada en GPU: 941.88 MB\n",
      "-----------------MEMORIA EN BATCH 9-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1868.00 MB\n",
      "Memoria asignada en GPU: 945.55 MB\n",
      "-----------------MEMORIA EN BATCH 10-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1902.00 MB\n",
      "Memoria asignada en GPU: 942.99 MB\n",
      "-----------------MEMORIA EN BATCH 11-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1794.00 MB\n",
      "Memoria asignada en GPU: 944.39 MB\n",
      "-----------------MEMORIA EN BATCH 12-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1648.00 MB\n",
      "Memoria asignada en GPU: 942.12 MB\n",
      "-----------------MEMORIA EN BATCH 13-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1728.00 MB\n",
      "Memoria asignada en GPU: 944.57 MB\n",
      "-----------------MEMORIA EN BATCH 14-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1250.00 MB\n",
      "Memoria asignada en GPU: 940.84 MB\n",
      "-----------------MEMORIA EN BATCH 15-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1290.00 MB\n",
      "Memoria asignada en GPU: 942.97 MB\n",
      "-----------------MEMORIA EN BATCH 16-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1442.00 MB\n",
      "Memoria asignada en GPU: 942.21 MB\n",
      "-----------------MEMORIA EN BATCH 17-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1322.00 MB\n",
      "Memoria asignada en GPU: 942.22 MB\n",
      "-----------------MEMORIA EN BATCH 18-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1620.00 MB\n",
      "Memoria asignada en GPU: 940.16 MB\n",
      "-----------------MEMORIA EN BATCH 19-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1726.00 MB\n",
      "Memoria asignada en GPU: 939.54 MB\n",
      "-----------------MEMORIA EN BATCH 20-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1526.00 MB\n",
      "Memoria asignada en GPU: 943.73 MB\n",
      "-----------------MEMORIA EN BATCH 21-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1472.00 MB\n",
      "Memoria asignada en GPU: 943.46 MB\n",
      "-----------------MEMORIA EN BATCH 22-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1688.00 MB\n",
      "Memoria asignada en GPU: 943.78 MB\n",
      "-----------------MEMORIA EN BATCH 23-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1370.00 MB\n",
      "Memoria asignada en GPU: 940.53 MB\n",
      "-----------------MEMORIA EN BATCH 24-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1552.00 MB\n",
      "Memoria asignada en GPU: 941.41 MB\n",
      "-----------------MEMORIA EN BATCH 25-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1486.00 MB\n",
      "Memoria asignada en GPU: 939.69 MB\n",
      "-----------------MEMORIA EN BATCH 26-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1458.00 MB\n",
      "Memoria asignada en GPU: 944.42 MB\n",
      "-----------------MEMORIA EN BATCH 27-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1486.00 MB\n",
      "Memoria asignada en GPU: 946.30 MB\n",
      "-----------------MEMORIA EN BATCH 28-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1530.00 MB\n",
      "Memoria asignada en GPU: 945.61 MB\n",
      "-----------------MEMORIA EN BATCH 29-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1484.00 MB\n",
      "Memoria asignada en GPU: 941.06 MB\n",
      "-----------------MEMORIA EN BATCH 30-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1736.00 MB\n",
      "Memoria asignada en GPU: 944.21 MB\n",
      "-----------------MEMORIA EN BATCH 31-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1770.00 MB\n",
      "Memoria asignada en GPU: 942.60 MB\n",
      "-----------------MEMORIA EN BATCH 32-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1748.00 MB\n",
      "Memoria asignada en GPU: 944.75 MB\n",
      "-----------------MEMORIA EN BATCH 33-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2174.00 MB\n",
      "Memoria asignada en GPU: 945.52 MB\n",
      "-----------------MEMORIA EN BATCH 34-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1930.00 MB\n",
      "Memoria asignada en GPU: 940.66 MB\n",
      "-----------------MEMORIA EN BATCH 35-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1990.00 MB\n",
      "Memoria asignada en GPU: 940.35 MB\n",
      "-----------------MEMORIA EN BATCH 36-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1966.00 MB\n",
      "Memoria asignada en GPU: 942.09 MB\n",
      "-----------------MEMORIA EN BATCH 37-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1860.00 MB\n",
      "Memoria asignada en GPU: 943.15 MB\n",
      "-----------------MEMORIA EN BATCH 38-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1810.00 MB\n",
      "Memoria asignada en GPU: 944.18 MB\n",
      "-----------------MEMORIA EN BATCH 39-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1792.00 MB\n",
      "Memoria asignada en GPU: 946.59 MB\n",
      "-----------------MEMORIA EN BATCH 40-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1796.00 MB\n",
      "Memoria asignada en GPU: 947.48 MB\n",
      "-----------------MEMORIA EN BATCH 41-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1478.00 MB\n",
      "Memoria asignada en GPU: 937.25 MB\n",
      "-----------------MEMORIA EN BATCH 42-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1576.00 MB\n",
      "Memoria asignada en GPU: 942.68 MB\n",
      "-----------------MEMORIA EN BATCH 43-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2058.00 MB\n",
      "Memoria asignada en GPU: 942.99 MB\n",
      "-----------------MEMORIA EN BATCH 44-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2050.00 MB\n",
      "Memoria asignada en GPU: 944.11 MB\n",
      "-----------------MEMORIA EN BATCH 45-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2066.00 MB\n",
      "Memoria asignada en GPU: 943.68 MB\n",
      "-----------------MEMORIA EN BATCH 46-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2050.00 MB\n",
      "Memoria asignada en GPU: 944.36 MB\n",
      "-----------------MEMORIA EN BATCH 47-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1934.00 MB\n",
      "Memoria asignada en GPU: 940.28 MB\n",
      "-----------------MEMORIA EN BATCH 48-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1566.00 MB\n",
      "Memoria asignada en GPU: 941.48 MB\n",
      "-----------------MEMORIA EN BATCH 49-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1600.00 MB\n",
      "Memoria asignada en GPU: 943.73 MB\n",
      "-----------------MEMORIA EN BATCH 50-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1360.00 MB\n",
      "Memoria asignada en GPU: 945.67 MB\n",
      "-----------------MEMORIA EN BATCH 51-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1392.00 MB\n",
      "Memoria asignada en GPU: 938.68 MB\n",
      "-----------------MEMORIA EN BATCH 52-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1638.00 MB\n",
      "Memoria asignada en GPU: 943.16 MB\n",
      "-----------------MEMORIA EN BATCH 53-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1538.00 MB\n",
      "Memoria asignada en GPU: 941.63 MB\n",
      "-----------------MEMORIA EN BATCH 54-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1764.00 MB\n",
      "Memoria asignada en GPU: 943.65 MB\n",
      "-----------------MEMORIA EN BATCH 55-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1448.00 MB\n",
      "Memoria asignada en GPU: 942.26 MB\n",
      "-----------------MEMORIA EN BATCH 56-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1446.00 MB\n",
      "Memoria asignada en GPU: 942.14 MB\n",
      "-----------------MEMORIA EN BATCH 57-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1426.00 MB\n",
      "Memoria asignada en GPU: 942.51 MB\n",
      "-----------------MEMORIA EN BATCH 58-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1454.00 MB\n",
      "Memoria asignada en GPU: 942.05 MB\n",
      "-----------------MEMORIA EN BATCH 59-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1452.00 MB\n",
      "Memoria asignada en GPU: 945.31 MB\n",
      "-----------------MEMORIA EN BATCH 60-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1620.00 MB\n",
      "Memoria asignada en GPU: 949.43 MB\n",
      "-----------------MEMORIA EN BATCH 61-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1680.00 MB\n",
      "Memoria asignada en GPU: 942.91 MB\n",
      "-----------------MEMORIA EN BATCH 62-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1680.00 MB\n",
      "Memoria asignada en GPU: 944.77 MB\n",
      "-----------------MEMORIA EN BATCH 63-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1486.00 MB\n",
      "Memoria asignada en GPU: 945.00 MB\n",
      "-----------------MEMORIA EN BATCH 64-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1266.00 MB\n",
      "Memoria asignada en GPU: 941.40 MB\n",
      "-----------------MEMORIA EN BATCH 65-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1352.00 MB\n",
      "Memoria asignada en GPU: 942.66 MB\n",
      "-----------------MEMORIA EN BATCH 66-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1458.00 MB\n",
      "Memoria asignada en GPU: 949.47 MB\n",
      "-----------------MEMORIA EN BATCH 67-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1498.00 MB\n",
      "Memoria asignada en GPU: 946.21 MB\n",
      "-----------------MEMORIA EN BATCH 68-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1428.00 MB\n",
      "Memoria asignada en GPU: 944.35 MB\n",
      "-----------------MEMORIA EN BATCH 69-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1372.00 MB\n",
      "Memoria asignada en GPU: 945.90 MB\n",
      "-----------------MEMORIA EN BATCH 70-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1714.00 MB\n",
      "Memoria asignada en GPU: 946.40 MB\n",
      "-----------------MEMORIA EN BATCH 71-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1672.00 MB\n",
      "Memoria asignada en GPU: 947.15 MB\n",
      "-----------------MEMORIA EN BATCH 72-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1972.00 MB\n",
      "Memoria asignada en GPU: 945.28 MB\n",
      "-----------------MEMORIA EN BATCH 73-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1924.00 MB\n",
      "Memoria asignada en GPU: 946.88 MB\n",
      "-----------------MEMORIA EN BATCH 74-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2020.00 MB\n",
      "Memoria asignada en GPU: 942.81 MB\n",
      "-----------------MEMORIA EN BATCH 75-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2014.00 MB\n",
      "Memoria asignada en GPU: 944.35 MB\n",
      "-----------------MEMORIA EN BATCH 76-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1854.00 MB\n",
      "Memoria asignada en GPU: 943.22 MB\n",
      "-----------------MEMORIA EN BATCH 77-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1850.00 MB\n",
      "Memoria asignada en GPU: 941.82 MB\n",
      "-----------------MEMORIA EN BATCH 78-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2050.00 MB\n",
      "Memoria asignada en GPU: 942.96 MB\n",
      "-----------------MEMORIA EN BATCH 79-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2116.00 MB\n",
      "Memoria asignada en GPU: 944.10 MB\n",
      "-----------------MEMORIA EN BATCH 80-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2068.00 MB\n",
      "Memoria asignada en GPU: 942.19 MB\n",
      "-----------------MEMORIA EN BATCH 81-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2088.00 MB\n",
      "Memoria asignada en GPU: 945.39 MB\n",
      "-----------------MEMORIA EN BATCH 82-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2096.00 MB\n",
      "Memoria asignada en GPU: 943.94 MB\n",
      "-----------------MEMORIA EN BATCH 83-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1912.00 MB\n",
      "Memoria asignada en GPU: 944.34 MB\n",
      "-----------------MEMORIA EN BATCH 84-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1844.00 MB\n",
      "Memoria asignada en GPU: 941.63 MB\n",
      "-----------------MEMORIA EN BATCH 85-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1860.00 MB\n",
      "Memoria asignada en GPU: 939.19 MB\n",
      "-----------------MEMORIA EN BATCH 86-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1604.00 MB\n",
      "Memoria asignada en GPU: 941.13 MB\n",
      "-----------------MEMORIA EN BATCH 87-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1970.00 MB\n",
      "Memoria asignada en GPU: 940.64 MB\n",
      "-----------------MEMORIA EN BATCH 88-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1984.00 MB\n",
      "Memoria asignada en GPU: 942.16 MB\n",
      "-----------------MEMORIA EN BATCH 89-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1986.00 MB\n",
      "Memoria asignada en GPU: 942.21 MB\n",
      "-----------------MEMORIA EN BATCH 90-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2076.00 MB\n",
      "Memoria asignada en GPU: 939.61 MB\n",
      "-----------------MEMORIA EN BATCH 91-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2076.00 MB\n",
      "Memoria asignada en GPU: 943.32 MB\n",
      "-----------------MEMORIA EN BATCH 92-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1700.00 MB\n",
      "Memoria asignada en GPU: 941.37 MB\n",
      "-----------------MEMORIA EN BATCH 93-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1730.00 MB\n",
      "Memoria asignada en GPU: 947.10 MB\n",
      "-----------------MEMORIA EN BATCH 94-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1754.00 MB\n",
      "Memoria asignada en GPU: 941.45 MB\n",
      "-----------------MEMORIA EN BATCH 95-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1946.00 MB\n",
      "Memoria asignada en GPU: 941.93 MB\n",
      "-----------------MEMORIA EN BATCH 96-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1466.00 MB\n",
      "Memoria asignada en GPU: 940.57 MB\n",
      "-----------------MEMORIA EN BATCH 97-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1432.00 MB\n",
      "Memoria asignada en GPU: 941.74 MB\n",
      "-----------------MEMORIA EN BATCH 98-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1376.00 MB\n",
      "Memoria asignada en GPU: 945.39 MB\n",
      "-----------------MEMORIA EN BATCH 99-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1398.00 MB\n",
      "Memoria asignada en GPU: 947.45 MB\n",
      "-----------------MEMORIA EN BATCH 100-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1462.00 MB\n",
      "Memoria asignada en GPU: 946.54 MB\n",
      "-----------------MEMORIA EN BATCH 101-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1758.00 MB\n",
      "Memoria asignada en GPU: 943.52 MB\n",
      "-----------------MEMORIA EN BATCH 102-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1726.00 MB\n",
      "Memoria asignada en GPU: 944.15 MB\n",
      "-----------------MEMORIA EN BATCH 103-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1794.00 MB\n",
      "Memoria asignada en GPU: 944.58 MB\n",
      "-----------------MEMORIA EN BATCH 104-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1718.00 MB\n",
      "Memoria asignada en GPU: 945.14 MB\n",
      "-----------------MEMORIA EN BATCH 105-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1740.00 MB\n",
      "Memoria asignada en GPU: 941.17 MB\n",
      "-----------------MEMORIA EN BATCH 106-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1764.00 MB\n",
      "Memoria asignada en GPU: 942.33 MB\n",
      "-----------------MEMORIA EN BATCH 107-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1828.00 MB\n",
      "Memoria asignada en GPU: 945.06 MB\n",
      "-----------------MEMORIA EN BATCH 108-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1854.00 MB\n",
      "Memoria asignada en GPU: 944.64 MB\n",
      "-----------------MEMORIA EN BATCH 109-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1806.00 MB\n",
      "Memoria asignada en GPU: 937.70 MB\n",
      "-----------------MEMORIA EN BATCH 110-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1862.00 MB\n",
      "Memoria asignada en GPU: 939.23 MB\n",
      "-----------------MEMORIA EN BATCH 111-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1800.00 MB\n",
      "Memoria asignada en GPU: 943.20 MB\n",
      "-----------------MEMORIA EN BATCH 112-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1788.00 MB\n",
      "Memoria asignada en GPU: 941.26 MB\n",
      "-----------------MEMORIA EN BATCH 113-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2064.00 MB\n",
      "Memoria asignada en GPU: 941.32 MB\n",
      "-----------------MEMORIA EN BATCH 114-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2012.00 MB\n",
      "Memoria asignada en GPU: 943.03 MB\n",
      "-----------------MEMORIA EN BATCH 115-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1584.00 MB\n",
      "Memoria asignada en GPU: 944.76 MB\n",
      "-----------------MEMORIA EN BATCH 116-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1562.00 MB\n",
      "Memoria asignada en GPU: 942.04 MB\n",
      "-----------------MEMORIA EN BATCH 117-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1838.00 MB\n",
      "Memoria asignada en GPU: 941.07 MB\n",
      "-----------------MEMORIA EN BATCH 118-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1784.00 MB\n",
      "Memoria asignada en GPU: 944.71 MB\n",
      "-----------------MEMORIA EN BATCH 119-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1636.00 MB\n",
      "Memoria asignada en GPU: 945.40 MB\n",
      "-----------------MEMORIA EN BATCH 120-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1602.00 MB\n",
      "Memoria asignada en GPU: 942.61 MB\n",
      "-----------------MEMORIA EN BATCH 121-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1614.00 MB\n",
      "Memoria asignada en GPU: 938.85 MB\n",
      "-----------------MEMORIA EN BATCH 122-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1640.00 MB\n",
      "Memoria asignada en GPU: 947.07 MB\n",
      "-----------------MEMORIA EN BATCH 123-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1814.00 MB\n",
      "Memoria asignada en GPU: 948.77 MB\n",
      "-----------------MEMORIA EN BATCH 124-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1778.00 MB\n",
      "Memoria asignada en GPU: 943.03 MB\n",
      "-----------------MEMORIA EN BATCH 125-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1778.00 MB\n",
      "Memoria asignada en GPU: 942.47 MB\n",
      "-----------------MEMORIA EN BATCH 126-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1798.00 MB\n",
      "Memoria asignada en GPU: 941.78 MB\n",
      "-----------------MEMORIA EN BATCH 127-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1776.00 MB\n",
      "Memoria asignada en GPU: 944.07 MB\n",
      "-----------------MEMORIA EN BATCH 128-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 1798.00 MB\n",
      "Memoria asignada en GPU: 945.53 MB\n",
      "-----------------MEMORIA EN BATCH 129-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2186.00 MB\n",
      "Memoria asignada en GPU: 946.61 MB\n",
      "-----------------MEMORIA EN BATCH 130-----------------\n",
      "Memoria total en GPU: 6143.69 MB\n",
      "Memoria reservada en GPU: 2074.00 MB\n",
      "Memoria asignada en GPU: 942.65 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     31\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iterator):\n\u001b[0;32m     34\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     35\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\data\\iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m             minibatch\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\data\\batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(x, name) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\data\\field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(batch)\n\u001b[1;32m--> 237\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumericalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\nicoc\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\data\\field.py:359\u001b[0m, in \u001b[0;36mField.numericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing(arr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 359\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[0;32m    362\u001b[0m     var\u001b[38;5;241m.\u001b[39mt_()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loaded_epoch = 0\n",
    "\n",
    "if load_model:\n",
    "    try:\n",
    "        checkpoint = torch.load(\"eng-esp-trained.tar\")\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        loaded_epoch = checkpoint[\"epoch\"]\n",
    "    except:\n",
    "        print(\"No se pudo cargar el modelo\")\n",
    "\n",
    "def save_and_eval(epoch):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"epoch\":epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        print(\"Guardando modelo...\")\n",
    "        torch.save(checkpoint, \"eng-esp-model.tar\")\n",
    "\n",
    "    model.eval()\n",
    "    score = bleu(test_data, n_eval, model)\n",
    "    print(\"Accuracy en test:\",score)\n",
    "    model.train()\n",
    "    return score\n",
    "\n",
    "acc = []\n",
    "for epoch in range(loaded_epoch, num_epochs):\n",
    "    acc.append(save_and_eval(epoch))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        batch_src = batch.src[:max_len,:].to(device)\n",
    "        batch_tgt = batch.tgt[:max_len,:].to(device)\n",
    "        \n",
    "        output = model(batch_src, batch_tgt[:-1, :])\n",
    "        delete_tensors('batch_src')\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        batch_tgt = batch_tgt[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, batch_tgt)\n",
    "        delete_tensors('batch_tgt','output')\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        del batch\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "acc.append(save_and_eval(num_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(range(num_epochs+1),acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
